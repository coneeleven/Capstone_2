{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using TensorFlow\n",
    "\n",
    "### Springboard Career Track Capstone Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquired from https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tar = tarfile.open(\"cifar-10-python.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to unpickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"  \n",
    "    from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(range(10))\n",
    "\n",
    "    return encoder.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign batch names, unpickle batches, and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = 'cifar-10-batches-py\\data_batch_1'\n",
    "f2 = 'cifar-10-batches-py\\data_batch_2'\n",
    "f3 = 'cifar-10-batches-py\\data_batch_3'\n",
    "f4 = 'cifar-10-batches-py\\data_batch_4'\n",
    "f5 = 'cifar-10-batches-py\\data_batch_5'\n",
    "x_test = 'cifar-10-batches-py\\\\test_batch'\n",
    "label_names = 'cifar-10-batches-py\\\\batches.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = unpickle(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y1 = b1.get(b'labels')\n",
    "b1data = b1.get(b'data')\n",
    "\n",
    "b2 = unpickle(f2)\n",
    "train_y2 = b2.get(b'labels')\n",
    "b2data = b2.get(b'data')\n",
    "\n",
    "b3 = unpickle(f3)\n",
    "train_y3 = b3.get(b'labels')\n",
    "b3data = b3.get(b'data')\n",
    "\n",
    "b4 = unpickle(f4)\n",
    "train_y4 = b4.get(b'labels')\n",
    "b4data = b4.get(b'data')\n",
    "\n",
    "b5 = unpickle(f5)\n",
    "train_y5 = b5.get(b'labels')\n",
    "b5data = b5.get(b'data')\n",
    "\n",
    "x_train = np.concatenate((b1data, b2data, b3data, b4data, b5data), axis=0)\n",
    "y_train = np.concatenate((train_y1, train_y2, train_y3, train_y4, train_y5), axis=0)\n",
    "\n",
    "y_train_cls = y_train\n",
    "y_train = one_hot_encode(y_train)\n",
    "\n",
    "test = unpickle(x_test)\n",
    "x_test = test.get(b'data')\n",
    "y_test_cls = test.get(b'labels')\n",
    "y_test = one_hot_encode(y_test_cls)\n",
    "label_names = unpickle(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "       [154, 126, 105, ..., 139, 142, 144],\n",
       "       [255, 253, 253, ...,  83,  83,  84],\n",
       "       ..., \n",
       "       [ 35,  40,  42, ...,  77,  66,  50],\n",
       "       [189, 186, 185, ..., 169, 171, 171],\n",
       "       [229, 236, 234, ..., 173, 162, 161]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_cases_per_batch': 10000,\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras (adapted from https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=(None, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Keras layers can be called on TensorFlow tensors:\n",
    "x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "from keras.objectives import categorical_crossentropy\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = x_train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: x_train,\n",
    "                                  labels: y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "batch_size = 500\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        j = 0\n",
    "        x_train_batch = x_train[j:j+batch_size,]\n",
    "        y_train_batch = y_train[j:j+batch_size,]\n",
    "        j = j + batch_size\n",
    "        if j > x_train.shape[0]:\n",
    "            j = 0\n",
    "            \n",
    "        train_step.run(feed_dict={img: x_train_batch,\n",
    "                                  labels: y_train_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print(acc_value.eval(feed_dict={img: x_test,\n",
    "                                    labels: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using baseline model Keras example from https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "606s - loss: 14.5166 - acc: 0.0994 - val_loss: 14.4660 - val_acc: 0.1025\n",
      "Epoch 2/10\n",
      "628s - loss: 14.5164 - acc: 0.0994 - val_loss: 14.4660 - val_acc: 0.1025\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2061c54f3265>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=10, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "49s - loss: 14.1651 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 2/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 3/10\n",
      "47s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 4/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 5/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 6/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 7/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 8/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 9/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n",
      "Epoch 10/10\n",
      "48s - loss: 14.4982 - acc: 0.1005 - val_loss: 14.5385 - val_acc: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2be28cfcd68>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 90.00%\n"
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=10, batch_size=1000, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "105s - loss: 2.3060 - acc: 0.1017 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 2/10\n",
      "104s - loss: 2.3027 - acc: 0.0982 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 3/10\n",
      "103s - loss: 2.3037 - acc: 0.0970 - val_loss: 2.3026 - val_acc: 0.1001\n",
      "Epoch 4/10\n",
      "103s - loss: 2.3032 - acc: 0.0982 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 5/10\n",
      "103s - loss: 2.3027 - acc: 0.0991 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 6/10\n",
      "105s - loss: 2.3027 - acc: 0.0981 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 7/10\n",
      "120s - loss: 2.3030 - acc: 0.0958 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-aa8f967c4ec3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbaseline_plus_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 867\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1598\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1599\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3   # 3072\n",
    "num_classes = 10\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# define baseline model\n",
    "def baseline_plus_hidden():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_plus_hidden()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "97s - loss: 2.2646 - acc: 0.1595 - val_loss: 2.1757 - val_acc: 0.2011\n",
      "Epoch 2/10\n",
      "91s - loss: 2.1155 - acc: 0.2015 - val_loss: 2.0545 - val_acc: 0.2521\n",
      "Epoch 3/10\n",
      "90s - loss: 2.0453 - acc: 0.2362 - val_loss: 1.9957 - val_acc: 0.2803\n",
      "Epoch 4/10\n",
      "90s - loss: 2.0002 - acc: 0.2583 - val_loss: 1.9531 - val_acc: 0.2946\n",
      "Epoch 5/10\n",
      "89s - loss: 1.9686 - acc: 0.2740 - val_loss: 1.9255 - val_acc: 0.3062\n",
      "Epoch 6/10\n",
      "89s - loss: 1.9516 - acc: 0.2825 - val_loss: 1.9076 - val_acc: 0.3120\n",
      "Epoch 7/10\n",
      "89s - loss: 1.9357 - acc: 0.2923 - val_loss: 1.8935 - val_acc: 0.3189\n",
      "Epoch 8/10\n",
      "89s - loss: 1.9218 - acc: 0.2991 - val_loss: 1.8787 - val_acc: 0.3208\n",
      "Epoch 9/10\n",
      "89s - loss: 1.9071 - acc: 0.3066 - val_loss: 1.8670 - val_acc: 0.3248\n",
      "Epoch 10/10\n",
      "90s - loss: 1.8976 - acc: 0.3110 - val_loss: 1.8522 - val_acc: 0.3332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a216e17b8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 66.68%\n"
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3   # 3072\n",
    "num_classes = 10\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# define baseline model\n",
    "def baseline_plus_hidden():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_plus_hidden()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "# from keras import backend as K\n",
    "# K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # load data\n",
    "# (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# # reshape to be [samples][pixels][width][height]\n",
    "# X_train = X_train.reshape(X_train.shape[0], 3, 32, 32).astype('float32')\n",
    "# X_test = X_test.reshape(X_test.shape[0], 3, 32, 32).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # normalize inputs from 0-255 to 0-1\n",
    "# x_train = x_train / 255\n",
    "# x_test = x_test / 255\n",
    "# # one hot encode outputs\n",
    "# y_train = np_utils.to_categorical(y_train)\n",
    "# y_test = np_utils.to_categorical(y_test)\n",
    "# num_classes = y_train.shape[1]\n",
    "# num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = x_train.reshape(50000, 32, 32, 3)\n",
    "X_test = x_test.reshape(10000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "67s - loss: 2.0858 - acc: 0.2374 - val_loss: 1.9498 - val_acc: 0.3084\n",
      "Epoch 2/10\n",
      "64s - loss: 1.8942 - acc: 0.3228 - val_loss: 1.8188 - val_acc: 0.3567\n",
      "Epoch 3/10\n",
      "71s - loss: 1.7938 - acc: 0.3618 - val_loss: 1.7492 - val_acc: 0.3727\n",
      "Epoch 4/10\n",
      "78s - loss: 1.7305 - acc: 0.3837 - val_loss: 1.6826 - val_acc: 0.4070\n",
      "Epoch 5/10\n",
      "75s - loss: 1.6815 - acc: 0.4017 - val_loss: 1.6410 - val_acc: 0.4181\n",
      "Epoch 6/10\n",
      "65s - loss: 1.6424 - acc: 0.4150 - val_loss: 1.5986 - val_acc: 0.4317\n",
      "Epoch 7/10\n",
      "64s - loss: 1.6049 - acc: 0.4249 - val_loss: 1.5761 - val_acc: 0.4405\n",
      "Epoch 8/10\n",
      "77s - loss: 1.5757 - acc: 0.4361 - val_loss: 1.5376 - val_acc: 0.4544\n",
      "Epoch 9/10\n",
      "83s - loss: 1.5490 - acc: 0.4449 - val_loss: 1.5118 - val_acc: 0.4647\n",
      "Epoch 10/10\n",
      "80s - loss: 1.5279 - acc: 0.4540 - val_loss: 1.4950 - val_acc: 0.4709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a2cdf59b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 52.91%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = convolution_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Convolution Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "121s - loss: 2.0900 - acc: 0.2266 - val_loss: 1.9423 - val_acc: 0.3001\n",
      "Epoch 2/100\n",
      "113s - loss: 1.9325 - acc: 0.2956 - val_loss: 1.8936 - val_acc: 0.3120\n",
      "Epoch 3/100\n",
      "124s - loss: 1.8715 - acc: 0.3237 - val_loss: 1.8090 - val_acc: 0.3490\n",
      "Epoch 4/100\n",
      "124s - loss: 1.8107 - acc: 0.3503 - val_loss: 1.7542 - val_acc: 0.3762\n",
      "Epoch 5/100\n",
      "127s - loss: 1.7631 - acc: 0.3670 - val_loss: 1.7163 - val_acc: 0.3847\n",
      "Epoch 6/100\n",
      "128s - loss: 1.7113 - acc: 0.3856 - val_loss: 1.6528 - val_acc: 0.4069\n",
      "Epoch 7/100\n",
      "114s - loss: 1.6704 - acc: 0.4022 - val_loss: 1.6225 - val_acc: 0.4219\n",
      "Epoch 8/100\n",
      "112s - loss: 1.6427 - acc: 0.4143 - val_loss: 1.6010 - val_acc: 0.4307\n",
      "Epoch 9/100\n",
      "112s - loss: 1.6166 - acc: 0.4235 - val_loss: 1.5680 - val_acc: 0.4386\n",
      "Epoch 10/100\n",
      "109s - loss: 1.5987 - acc: 0.4279 - val_loss: 1.5679 - val_acc: 0.4404\n",
      "Epoch 11/100\n",
      "107s - loss: 1.5783 - acc: 0.4367 - val_loss: 1.5380 - val_acc: 0.4490\n",
      "Epoch 12/100\n",
      "107s - loss: 1.5611 - acc: 0.4428 - val_loss: 1.5351 - val_acc: 0.4537\n",
      "Epoch 13/100\n",
      "113s - loss: 1.5456 - acc: 0.4472 - val_loss: 1.5219 - val_acc: 0.4535\n",
      "Epoch 14/100\n",
      "118s - loss: 1.5297 - acc: 0.4529 - val_loss: 1.4917 - val_acc: 0.4668\n",
      "Epoch 15/100\n",
      "112s - loss: 1.5190 - acc: 0.4559 - val_loss: 1.4885 - val_acc: 0.4704\n",
      "Epoch 16/100\n",
      "108s - loss: 1.5027 - acc: 0.4611 - val_loss: 1.4748 - val_acc: 0.4760\n",
      "Epoch 17/100\n",
      "116s - loss: 1.4885 - acc: 0.4675 - val_loss: 1.4619 - val_acc: 0.4782\n",
      "Epoch 18/100\n",
      "137s - loss: 1.4772 - acc: 0.4724 - val_loss: 1.4579 - val_acc: 0.4736\n",
      "Epoch 19/100\n",
      "135s - loss: 1.4644 - acc: 0.4765 - val_loss: 1.4442 - val_acc: 0.4818\n",
      "Epoch 20/100\n",
      "123s - loss: 1.4529 - acc: 0.4805 - val_loss: 1.4358 - val_acc: 0.4857\n",
      "Epoch 21/100\n",
      "124s - loss: 1.4437 - acc: 0.4822 - val_loss: 1.4291 - val_acc: 0.4903\n",
      "Epoch 22/100\n",
      "139s - loss: 1.4306 - acc: 0.4883 - val_loss: 1.4128 - val_acc: 0.4930\n",
      "Epoch 23/100\n",
      "146s - loss: 1.4202 - acc: 0.4930 - val_loss: 1.4292 - val_acc: 0.4911\n",
      "Epoch 24/100\n",
      "125s - loss: 1.4126 - acc: 0.4938 - val_loss: 1.3992 - val_acc: 0.5010\n",
      "Epoch 25/100\n",
      "130s - loss: 1.4041 - acc: 0.4987 - val_loss: 1.4085 - val_acc: 0.4965\n",
      "Epoch 26/100\n",
      "130s - loss: 1.3960 - acc: 0.5000 - val_loss: 1.3840 - val_acc: 0.5055\n",
      "Epoch 27/100\n",
      "123s - loss: 1.3867 - acc: 0.5035 - val_loss: 1.3846 - val_acc: 0.5042\n",
      "Epoch 28/100\n",
      "126s - loss: 1.3780 - acc: 0.5092 - val_loss: 1.3785 - val_acc: 0.5108\n",
      "Epoch 29/100\n",
      "120s - loss: 1.3727 - acc: 0.5095 - val_loss: 1.3648 - val_acc: 0.5135\n",
      "Epoch 30/100\n",
      "118s - loss: 1.3623 - acc: 0.5134 - val_loss: 1.3547 - val_acc: 0.5182\n",
      "Epoch 31/100\n",
      "128s - loss: 1.3539 - acc: 0.5170 - val_loss: 1.3619 - val_acc: 0.5166\n",
      "Epoch 32/100\n",
      "130s - loss: 1.3464 - acc: 0.5210 - val_loss: 1.3391 - val_acc: 0.5251\n",
      "Epoch 33/100\n",
      "128s - loss: 1.3355 - acc: 0.5216 - val_loss: 1.3405 - val_acc: 0.5229\n",
      "Epoch 34/100\n",
      "119s - loss: 1.3318 - acc: 0.5244 - val_loss: 1.3394 - val_acc: 0.5231\n",
      "Epoch 35/100\n",
      "120s - loss: 1.3216 - acc: 0.5291 - val_loss: 1.3441 - val_acc: 0.5178\n",
      "Epoch 36/100\n",
      "123s - loss: 1.3139 - acc: 0.5338 - val_loss: 1.3174 - val_acc: 0.5286\n",
      "Epoch 37/100\n",
      "122s - loss: 1.3057 - acc: 0.5328 - val_loss: 1.3135 - val_acc: 0.5320\n",
      "Epoch 38/100\n",
      "115s - loss: 1.2963 - acc: 0.5359 - val_loss: 1.3217 - val_acc: 0.5309\n",
      "Epoch 39/100\n",
      "122s - loss: 1.2934 - acc: 0.5397 - val_loss: 1.3011 - val_acc: 0.5360\n",
      "Epoch 40/100\n",
      "120s - loss: 1.2843 - acc: 0.5415 - val_loss: 1.3037 - val_acc: 0.5343\n",
      "Epoch 41/100\n",
      "121s - loss: 1.2824 - acc: 0.5421 - val_loss: 1.2999 - val_acc: 0.5382\n",
      "Epoch 42/100\n",
      "119s - loss: 1.2752 - acc: 0.5451 - val_loss: 1.3140 - val_acc: 0.5342\n",
      "Epoch 43/100\n",
      "118s - loss: 1.2713 - acc: 0.5462 - val_loss: 1.2865 - val_acc: 0.5420\n",
      "Epoch 44/100\n",
      "118s - loss: 1.2597 - acc: 0.5488 - val_loss: 1.2864 - val_acc: 0.5402\n",
      "Epoch 45/100\n",
      "118s - loss: 1.2579 - acc: 0.5505 - val_loss: 1.2841 - val_acc: 0.5441\n",
      "Epoch 46/100\n",
      "123s - loss: 1.2522 - acc: 0.5508 - val_loss: 1.2752 - val_acc: 0.5461\n",
      "Epoch 47/100\n",
      "115s - loss: 1.2453 - acc: 0.5548 - val_loss: 1.2681 - val_acc: 0.5474\n",
      "Epoch 48/100\n",
      "119s - loss: 1.2363 - acc: 0.5587 - val_loss: 1.2745 - val_acc: 0.5458\n",
      "Epoch 49/100\n",
      "118s - loss: 1.2338 - acc: 0.5589 - val_loss: 1.2745 - val_acc: 0.5492\n",
      "Epoch 50/100\n",
      "117s - loss: 1.2265 - acc: 0.5623 - val_loss: 1.2623 - val_acc: 0.5537\n",
      "Epoch 51/100\n",
      "120s - loss: 1.2303 - acc: 0.5599 - val_loss: 1.2785 - val_acc: 0.5499\n",
      "Epoch 52/100\n",
      "121s - loss: 1.2184 - acc: 0.5651 - val_loss: 1.2629 - val_acc: 0.5492\n",
      "Epoch 53/100\n",
      "139s - loss: 1.2157 - acc: 0.5665 - val_loss: 1.2515 - val_acc: 0.5540\n",
      "Epoch 54/100\n",
      "126s - loss: 1.2092 - acc: 0.5663 - val_loss: 1.2508 - val_acc: 0.5552\n",
      "Epoch 55/100\n",
      "124s - loss: 1.2069 - acc: 0.5688 - val_loss: 1.2449 - val_acc: 0.5569\n",
      "Epoch 56/100\n",
      "154s - loss: 1.2003 - acc: 0.5714 - val_loss: 1.2441 - val_acc: 0.5576\n",
      "Epoch 57/100\n",
      "145s - loss: 1.1947 - acc: 0.5736 - val_loss: 1.2542 - val_acc: 0.5538\n",
      "Epoch 58/100\n",
      "141s - loss: 1.1917 - acc: 0.5729 - val_loss: 1.2417 - val_acc: 0.5560\n",
      "Epoch 59/100\n",
      "147s - loss: 1.1891 - acc: 0.5760 - val_loss: 1.2475 - val_acc: 0.5546\n",
      "Epoch 60/100\n",
      "140s - loss: 1.1871 - acc: 0.5746 - val_loss: 1.2517 - val_acc: 0.5546\n",
      "Epoch 61/100\n",
      "144s - loss: 1.1814 - acc: 0.5799 - val_loss: 1.2279 - val_acc: 0.5614\n",
      "Epoch 62/100\n",
      "139s - loss: 1.1744 - acc: 0.5794 - val_loss: 1.2351 - val_acc: 0.5584\n",
      "Epoch 63/100\n",
      "139s - loss: 1.1676 - acc: 0.5831 - val_loss: 1.2301 - val_acc: 0.5628\n",
      "Epoch 64/100\n",
      "135s - loss: 1.1667 - acc: 0.5826 - val_loss: 1.2593 - val_acc: 0.5528\n",
      "Epoch 65/100\n",
      "129s - loss: 1.1614 - acc: 0.5844 - val_loss: 1.2509 - val_acc: 0.5555\n",
      "Epoch 66/100\n",
      "158s - loss: 1.1647 - acc: 0.5834 - val_loss: 1.2338 - val_acc: 0.5597\n",
      "Epoch 67/100\n",
      "123s - loss: 1.1564 - acc: 0.5884 - val_loss: 1.2303 - val_acc: 0.5615\n",
      "Epoch 68/100\n",
      "121s - loss: 1.1505 - acc: 0.5906 - val_loss: 1.2216 - val_acc: 0.5631\n",
      "Epoch 69/100\n",
      "120s - loss: 1.1486 - acc: 0.5897 - val_loss: 1.2219 - val_acc: 0.5643\n",
      "Epoch 70/100\n",
      "124s - loss: 1.1457 - acc: 0.5923 - val_loss: 1.2211 - val_acc: 0.5657\n",
      "Epoch 71/100\n",
      "127s - loss: 1.1402 - acc: 0.5951 - val_loss: 1.2128 - val_acc: 0.5691\n",
      "Epoch 72/100\n",
      "124s - loss: 1.1370 - acc: 0.5950 - val_loss: 1.2183 - val_acc: 0.5671\n",
      "Epoch 73/100\n",
      "125s - loss: 1.1331 - acc: 0.5961 - val_loss: 1.2345 - val_acc: 0.5604\n",
      "Epoch 74/100\n",
      "122s - loss: 1.1298 - acc: 0.5961 - val_loss: 1.2149 - val_acc: 0.5670\n",
      "Epoch 75/100\n",
      "120s - loss: 1.1253 - acc: 0.5979 - val_loss: 1.2088 - val_acc: 0.5711\n",
      "Epoch 76/100\n",
      "135s - loss: 1.1233 - acc: 0.6007 - val_loss: 1.2141 - val_acc: 0.5682\n",
      "Epoch 77/100\n",
      "142s - loss: 1.1252 - acc: 0.5971 - val_loss: 1.2048 - val_acc: 0.5715\n",
      "Epoch 78/100\n",
      "137s - loss: 1.1172 - acc: 0.6022 - val_loss: 1.2099 - val_acc: 0.5669\n",
      "Epoch 79/100\n",
      "129s - loss: 1.1125 - acc: 0.6017 - val_loss: 1.2256 - val_acc: 0.5654\n",
      "Epoch 80/100\n",
      "119s - loss: 1.1104 - acc: 0.6047 - val_loss: 1.2152 - val_acc: 0.5702\n",
      "Epoch 81/100\n",
      "126s - loss: 1.1069 - acc: 0.6039 - val_loss: 1.2006 - val_acc: 0.5722\n",
      "Epoch 82/100\n",
      "134s - loss: 1.1024 - acc: 0.6067 - val_loss: 1.2042 - val_acc: 0.5733\n",
      "Epoch 83/100\n",
      "121s - loss: 1.0978 - acc: 0.6091 - val_loss: 1.2092 - val_acc: 0.5732\n",
      "Epoch 84/100\n",
      "125s - loss: 1.0977 - acc: 0.6080 - val_loss: 1.2022 - val_acc: 0.5710\n",
      "Epoch 85/100\n",
      "139s - loss: 1.0956 - acc: 0.6082 - val_loss: 1.2351 - val_acc: 0.5623\n",
      "Epoch 86/100\n",
      "138s - loss: 1.0941 - acc: 0.6076 - val_loss: 1.2014 - val_acc: 0.5747\n",
      "Epoch 87/100\n",
      "139s - loss: 1.0905 - acc: 0.6094 - val_loss: 1.2015 - val_acc: 0.5718\n",
      "Epoch 88/100\n",
      "138s - loss: 1.0888 - acc: 0.6118 - val_loss: 1.2017 - val_acc: 0.5735\n",
      "Epoch 89/100\n",
      "138s - loss: 1.0821 - acc: 0.6125 - val_loss: 1.2084 - val_acc: 0.5711\n",
      "Epoch 90/100\n",
      "137s - loss: 1.0759 - acc: 0.6189 - val_loss: 1.2157 - val_acc: 0.5708\n",
      "Epoch 91/100\n",
      "138s - loss: 1.0756 - acc: 0.6161 - val_loss: 1.2239 - val_acc: 0.5655\n",
      "Epoch 92/100\n",
      "139s - loss: 1.0720 - acc: 0.6147 - val_loss: 1.2212 - val_acc: 0.5659\n",
      "Epoch 93/100\n",
      "139s - loss: 1.0746 - acc: 0.6155 - val_loss: 1.1976 - val_acc: 0.5706\n",
      "Epoch 94/100\n",
      "138s - loss: 1.0715 - acc: 0.6171 - val_loss: 1.2096 - val_acc: 0.5685\n",
      "Epoch 95/100\n",
      "137s - loss: 1.0701 - acc: 0.6190 - val_loss: 1.2040 - val_acc: 0.5734\n",
      "Epoch 96/100\n",
      "142s - loss: 1.0668 - acc: 0.6204 - val_loss: 1.2007 - val_acc: 0.5739\n",
      "Epoch 97/100\n",
      "142s - loss: 1.0662 - acc: 0.6200 - val_loss: 1.2065 - val_acc: 0.5703\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142s - loss: 1.0560 - acc: 0.6214 - val_loss: 1.1992 - val_acc: 0.5745\n",
      "Epoch 99/100\n",
      "142s - loss: 1.0582 - acc: 0.6199 - val_loss: 1.1968 - val_acc: 0.5788\n",
      "Epoch 100/100\n",
      "141s - loss: 1.0545 - acc: 0.6236 - val_loss: 1.1952 - val_acc: 0.5790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a3df9c5c0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Error with 2 hidden layers: 42.10%\n"
     ]
    }
   ],
   "source": [
    "def convolution2_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = convolution2_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=200, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Convolution Error with 2 hidden layers: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "50000/50000 [==============================] - 121s - loss: 2.1601 - acc: 0.1895 - val_loss: 1.9577 - val_acc: 0.3027\n",
      "Epoch 2/100\n",
      "50000/50000 [==============================] - 119s - loss: 1.9034 - acc: 0.3120 - val_loss: 1.8156 - val_acc: 0.3453\n",
      "Epoch 3/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.8069 - acc: 0.3484 - val_loss: 1.7329 - val_acc: 0.3801\n",
      "Epoch 4/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.7427 - acc: 0.3728 - val_loss: 1.6711 - val_acc: 0.4038\n",
      "Epoch 5/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.6954 - acc: 0.3920 - val_loss: 1.6296 - val_acc: 0.4156\n",
      "Epoch 6/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.6609 - acc: 0.4050 - val_loss: 1.6083 - val_acc: 0.4239\n",
      "Epoch 7/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.6316 - acc: 0.4154 - val_loss: 1.5842 - val_acc: 0.4345\n",
      "Epoch 8/100\n",
      "50000/50000 [==============================] - 113s - loss: 1.6075 - acc: 0.4249 - val_loss: 1.5519 - val_acc: 0.4493\n",
      "Epoch 9/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.5877 - acc: 0.4307 - val_loss: 1.5301 - val_acc: 0.4584\n",
      "Epoch 10/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.5700 - acc: 0.4387 - val_loss: 1.5154 - val_acc: 0.4615\n",
      "Epoch 11/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.5484 - acc: 0.4483 - val_loss: 1.4962 - val_acc: 0.4689\n",
      "Epoch 12/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.5320 - acc: 0.4531 - val_loss: 1.4845 - val_acc: 0.4735\n",
      "Epoch 13/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.5193 - acc: 0.4594 - val_loss: 1.4817 - val_acc: 0.4707\n",
      "Epoch 14/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.5048 - acc: 0.4650 - val_loss: 1.4640 - val_acc: 0.4836\n",
      "Epoch 15/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.4918 - acc: 0.4678 - val_loss: 1.4462 - val_acc: 0.4928\n",
      "Epoch 16/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.4779 - acc: 0.4723 - val_loss: 1.4428 - val_acc: 0.4934\n",
      "Epoch 17/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.4696 - acc: 0.4746 - val_loss: 1.4322 - val_acc: 0.4960\n",
      "Epoch 18/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.4556 - acc: 0.4818 - val_loss: 1.4203 - val_acc: 0.4952\n",
      "Epoch 19/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.4469 - acc: 0.4848 - val_loss: 1.4140 - val_acc: 0.5031\n",
      "Epoch 20/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.4387 - acc: 0.4854 - val_loss: 1.4079 - val_acc: 0.5013\n",
      "Epoch 21/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.4291 - acc: 0.4906 - val_loss: 1.3913 - val_acc: 0.5087\n",
      "Epoch 22/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.4183 - acc: 0.4959 - val_loss: 1.3889 - val_acc: 0.5099\n",
      "Epoch 23/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.4113 - acc: 0.4959 - val_loss: 1.3917 - val_acc: 0.5010\n",
      "Epoch 24/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.4048 - acc: 0.4984 - val_loss: 1.3701 - val_acc: 0.5167\n",
      "Epoch 25/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3970 - acc: 0.5038 - val_loss: 1.3650 - val_acc: 0.5178\n",
      "Epoch 26/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.3908 - acc: 0.5052 - val_loss: 1.3598 - val_acc: 0.5206\n",
      "Epoch 27/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3814 - acc: 0.5087 - val_loss: 1.3562 - val_acc: 0.5212\n",
      "Epoch 28/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3757 - acc: 0.5107 - val_loss: 1.3478 - val_acc: 0.5266\n",
      "Epoch 29/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.3709 - acc: 0.5142 - val_loss: 1.3448 - val_acc: 0.5245\n",
      "Epoch 30/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.3643 - acc: 0.5152 - val_loss: 1.3448 - val_acc: 0.5230\n",
      "Epoch 31/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.3557 - acc: 0.5169 - val_loss: 1.3319 - val_acc: 0.5269\n",
      "Epoch 32/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.3486 - acc: 0.5189 - val_loss: 1.3282 - val_acc: 0.5261\n",
      "Epoch 33/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3466 - acc: 0.5209 - val_loss: 1.3302 - val_acc: 0.5241\n",
      "Epoch 34/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.3383 - acc: 0.5238 - val_loss: 1.3212 - val_acc: 0.5291\n",
      "Epoch 35/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3325 - acc: 0.5271 - val_loss: 1.3238 - val_acc: 0.5270\n",
      "Epoch 36/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.3292 - acc: 0.5265 - val_loss: 1.3109 - val_acc: 0.5334\n",
      "Epoch 37/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3216 - acc: 0.5304 - val_loss: 1.3219 - val_acc: 0.5349\n",
      "Epoch 38/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3183 - acc: 0.5343 - val_loss: 1.3072 - val_acc: 0.5348\n",
      "Epoch 39/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3139 - acc: 0.5339 - val_loss: 1.3073 - val_acc: 0.5330\n",
      "Epoch 40/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.3087 - acc: 0.5369 - val_loss: 1.3008 - val_acc: 0.5372\n",
      "Epoch 41/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.3091 - acc: 0.5337 - val_loss: 1.3001 - val_acc: 0.5387\n",
      "Epoch 42/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.3040 - acc: 0.5380 - val_loss: 1.2966 - val_acc: 0.5388\n",
      "Epoch 43/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.2969 - acc: 0.5409 - val_loss: 1.2975 - val_acc: 0.5329\n",
      "Epoch 44/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.2916 - acc: 0.5435 - val_loss: 1.2938 - val_acc: 0.5386\n",
      "Epoch 45/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.2854 - acc: 0.5435 - val_loss: 1.2833 - val_acc: 0.5446\n",
      "Epoch 46/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.2848 - acc: 0.5456 - val_loss: 1.2870 - val_acc: 0.5426\n",
      "Epoch 47/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.2815 - acc: 0.5448 - val_loss: 1.2871 - val_acc: 0.5433\n",
      "Epoch 48/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.2784 - acc: 0.5466 - val_loss: 1.2882 - val_acc: 0.5454\n",
      "Epoch 49/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.2733 - acc: 0.5483 - val_loss: 1.2806 - val_acc: 0.5474\n",
      "Epoch 50/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.2733 - acc: 0.5470 - val_loss: 1.2813 - val_acc: 0.5475\n",
      "Epoch 51/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.2666 - acc: 0.5474 - val_loss: 1.2733 - val_acc: 0.551354\n",
      "Epoch 52/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.2641 - acc: 0.5514 - val_loss: 1.2762 - val_acc: 0.5486\n",
      "Epoch 53/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.2551 - acc: 0.5545 - val_loss: 1.2754 - val_acc: 0.5476\n",
      "Epoch 54/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.2577 - acc: 0.5538 - val_loss: 1.2706 - val_acc: 0.5503\n",
      "Epoch 55/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.2562 - acc: 0.5524 - val_loss: 1.2660 - val_acc: 0.5557\n",
      "Epoch 56/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.2522 - acc: 0.5556 - val_loss: 1.2645 - val_acc: 0.5539\n",
      "Epoch 57/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.2502 - acc: 0.5547 - val_loss: 1.2636 - val_acc: 0.5530\n",
      "Epoch 58/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.2438 - acc: 0.5560 - val_loss: 1.2634 - val_acc: 0.5521\n",
      "Epoch 59/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.2461 - acc: 0.5589 - val_loss: 1.2646 - val_acc: 0.5533\n",
      "Epoch 60/100\n",
      "50000/50000 [==============================] - 118s - loss: 1.2396 - acc: 0.5598 - val_loss: 1.2747 - val_acc: 0.5526\n",
      "Epoch 61/100\n",
      "50000/50000 [==============================] - 118s - loss: 1.2387 - acc: 0.5606 - val_loss: 1.2707 - val_acc: 0.5535\n",
      "Epoch 62/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.2363 - acc: 0.5604 - val_loss: 1.2676 - val_acc: 0.5539\n",
      "Epoch 63/100\n",
      "50000/50000 [==============================] - 112s - loss: 1.2336 - acc: 0.5626 - val_loss: 1.2612 - val_acc: 0.5504\n",
      "Epoch 64/100\n",
      "50000/50000 [==============================] - 111s - loss: 1.2276 - acc: 0.5628 - val_loss: 1.2540 - val_acc: 0.5581\n",
      "Epoch 65/100\n",
      "50000/50000 [==============================] - 111s - loss: 1.2268 - acc: 0.5634 - val_loss: 1.2509 - val_acc: 0.5604\n",
      "Epoch 66/100\n",
      "50000/50000 [==============================] - 111s - loss: 1.2239 - acc: 0.5652 - val_loss: 1.2536 - val_acc: 0.5547\n",
      "Epoch 67/100\n",
      "50000/50000 [==============================] - 112s - loss: 1.2256 - acc: 0.5656 - val_loss: 1.2536 - val_acc: 0.5532\n",
      "Epoch 68/100\n",
      "50000/50000 [==============================] - 112s - loss: 1.2208 - acc: 0.5669 - val_loss: 1.2563 - val_acc: 0.5573- E - ETA: \n",
      "Epoch 69/100\n",
      "50000/50000 [==============================] - 111s - loss: 1.2185 - acc: 0.5673 - val_loss: 1.2548 - val_acc: 0.5564\n",
      "Epoch 70/100\n",
      "50000/50000 [==============================] - 112s - loss: 1.2137 - acc: 0.5665 - val_loss: 1.2627 - val_acc: 0.5580\n",
      "Epoch 71/100\n",
      "50000/50000 [==============================] - 112s - loss: 1.2137 - acc: 0.5653 - val_loss: 1.2536 - val_acc: 0.5564\n",
      "Epoch 72/100\n",
      "50000/50000 [==============================] - 112s - loss: 1.2122 - acc: 0.5691 - val_loss: 1.2492 - val_acc: 0.5591\n",
      "Epoch 73/100\n",
      "50000/50000 [==============================] - 111s - loss: 1.2114 - acc: 0.5694 - val_loss: 1.2449 - val_acc: 0.5593\n",
      "Epoch 74/100\n",
      "50000/50000 [==============================] - 111s - loss: 1.2067 - acc: 0.5726 - val_loss: 1.2513 - val_acc: 0.5563\n",
      "Epoch 75/100\n",
      "50000/50000 [==============================] - 111s - loss: 1.2068 - acc: 0.5714 - val_loss: 1.2483 - val_acc: 0.5567\n",
      "Epoch 76/100\n",
      "50000/50000 [==============================] - 113s - loss: 1.2011 - acc: 0.5742 - val_loss: 1.2452 - val_acc: 0.5603\n",
      "Epoch 77/100\n",
      "50000/50000 [==============================] - 113s - loss: 1.2041 - acc: 0.5702 - val_loss: 1.2424 - val_acc: 0.5625\n",
      "Epoch 78/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.1984 - acc: 0.5730 - val_loss: 1.2385 - val_acc: 0.5616\n",
      "Epoch 79/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1962 - acc: 0.5756 - val_loss: 1.2514 - val_acc: 0.5611\n",
      "Epoch 80/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1969 - acc: 0.5748 - val_loss: 1.2459 - val_acc: 0.5609\n",
      "Epoch 81/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.1931 - acc: 0.5745 - val_loss: 1.2416 - val_acc: 0.5622\n",
      "Epoch 82/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1939 - acc: 0.5746 - val_loss: 1.2406 - val_acc: 0.5645\n",
      "Epoch 83/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1885 - acc: 0.5767 - val_loss: 1.2438 - val_acc: 0.5605\n",
      "Epoch 84/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1887 - acc: 0.5767 - val_loss: 1.2528 - val_acc: 0.5625\n",
      "Epoch 85/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1857 - acc: 0.5795 - val_loss: 1.2505 - val_acc: 0.5593\n",
      "Epoch 86/100\n",
      "50000/50000 [==============================] - 114s - loss: 1.1853 - acc: 0.5797 - val_loss: 1.2555 - val_acc: 0.5569\n",
      "Epoch 87/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1825 - acc: 0.5794 - val_loss: 1.2410 - val_acc: 0.5625\n",
      "Epoch 88/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1812 - acc: 0.5791 - val_loss: 1.2499 - val_acc: 0.5553\n",
      "Epoch 89/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.1807 - acc: 0.5785 - val_loss: 1.2465 - val_acc: 0.5565\n",
      "Epoch 90/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.1791 - acc: 0.5818 - val_loss: 1.2493 - val_acc: 0.5624\n",
      "Epoch 91/100\n",
      "50000/50000 [==============================] - 116s - loss: 1.1792 - acc: 0.5793 - val_loss: 1.2455 - val_acc: 0.5601\n",
      "Epoch 92/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.1739 - acc: 0.5835 - val_loss: 1.2401 - val_acc: 0.5611\n",
      "Epoch 93/100\n",
      "50000/50000 [==============================] - 117s - loss: 1.1739 - acc: 0.5822 - val_loss: 1.2483 - val_acc: 0.5556\n",
      "Epoch 94/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1702 - acc: 0.5848 - val_loss: 1.2371 - val_acc: 0.5625\n",
      "Epoch 95/100\n",
      "50000/50000 [==============================] - 119s - loss: 1.1710 - acc: 0.5813 - val_loss: 1.2421 - val_acc: 0.5593\n",
      "Epoch 96/100\n",
      "50000/50000 [==============================] - 120s - loss: 1.1712 - acc: 0.5826 - val_loss: 1.2423 - val_acc: 0.5594\n",
      "Epoch 97/100\n",
      "50000/50000 [==============================] - 105s - loss: 1.1654 - acc: 0.5845 - val_loss: 1.2382 - val_acc: 0.5627\n",
      "Epoch 98/100\n",
      "50000/50000 [==============================] - 119s - loss: 1.1664 - acc: 0.5837 - val_loss: 1.2420 - val_acc: 0.5593\n",
      "Epoch 99/100\n",
      "50000/50000 [==============================] - 115s - loss: 1.1670 - acc: 0.5849 - val_loss: 1.2461 - val_acc: 0.5587\n",
      "Epoch 100/100\n",
      "50000/50000 [==============================] - 120s - loss: 1.1621 - acc: 0.5851 - val_loss: 1.2311 - val_acc: 0.5646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a4c42d198>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0sConvolution Error with 2 hidden layers: 43.54%\n"
     ]
    }
   ],
   "source": [
    "def convolution2b_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = convolution2b_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=100, verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Convolution Error with 2 hidden layers: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "45000/45000 [==============================] - 79s - loss: 2.2802 - acc: 0.1318 - val_loss: 2.1988 - val_acc: 0.2144\n",
      "Epoch 2/20\n",
      "45000/45000 [==============================] - 71s - loss: 2.0982 - acc: 0.2379 - val_loss: 2.0283 - val_acc: 0.2706\n",
      "Epoch 3/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.9941 - acc: 0.2813 - val_loss: 1.9567 - val_acc: 0.2958\n",
      "Epoch 4/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.9303 - acc: 0.3058 - val_loss: 1.8910 - val_acc: 0.3214\n",
      "Epoch 5/20\n",
      "45000/45000 [==============================] - 71s - loss: 1.8735 - acc: 0.3279 - val_loss: 1.8422 - val_acc: 0.3382\n",
      "Epoch 6/20\n",
      "45000/45000 [==============================] - 71s - loss: 1.8186 - acc: 0.3486 - val_loss: 1.7939 - val_acc: 0.3574\n",
      "Epoch 7/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.7771 - acc: 0.3642 - val_loss: 1.7535 - val_acc: 0.3640\n",
      "Epoch 8/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.7461 - acc: 0.3751 - val_loss: 1.7275 - val_acc: 0.3770\n",
      "Epoch 9/20\n",
      "45000/45000 [==============================] - 71s - loss: 1.7238 - acc: 0.3838 - val_loss: 1.7043 - val_acc: 0.3820\n",
      "Epoch 10/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.7037 - acc: 0.3910 - val_loss: 1.6840 - val_acc: 0.3970\n",
      "Epoch 11/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.6900 - acc: 0.3964 - val_loss: 1.6719 - val_acc: 0.3982\n",
      "Epoch 12/20\n",
      "45000/45000 [==============================] - 71s - loss: 1.6759 - acc: 0.4025 - val_loss: 1.6552 - val_acc: 0.4024\n",
      "Epoch 13/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.6653 - acc: 0.4043 - val_loss: 1.6477 - val_acc: 0.4036\n",
      "Epoch 14/20\n",
      "45000/45000 [==============================] - 71s - loss: 1.6559 - acc: 0.4088 - val_loss: 1.6298 - val_acc: 0.4170\n",
      "Epoch 15/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.6410 - acc: 0.4129 - val_loss: 1.6134 - val_acc: 0.4210\n",
      "Epoch 16/20\n",
      "45000/45000 [==============================] - 71s - loss: 1.6294 - acc: 0.4199 - val_loss: 1.6167 - val_acc: 0.4140\n",
      "Epoch 17/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.6238 - acc: 0.4205 - val_loss: 1.5961 - val_acc: 0.4266\n",
      "Epoch 18/20\n",
      "45000/45000 [==============================] - 70s - loss: 1.6111 - acc: 0.4244 - val_loss: 1.5826 - val_acc: 0.4362\n",
      "Epoch 19/20\n",
      "45000/45000 [==============================] - 71s - loss: 1.6005 - acc: 0.4296 - val_loss: 1.5819 - val_acc: 0.4308\n",
      "Epoch 20/20\n",
      "45000/45000 [==============================] - 69s - loss: 1.5933 - acc: 0.4326 - val_loss: 1.5640 - val_acc: 0.4374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23a55646278>]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9984/10000 [============================>.] - ETA: 0sConvolution Error with 2 hidden layers: 55.09%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHVCAYAAADywj0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23a52a949b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convolution2b_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = convolution2b_model()\n",
    "# Fit the model\n",
    "results = model.fit(X_train, y_train, validation_split=0.1, epochs=20, batch_size=1000, verbose=1)\n",
    "plt.plot(results.history['loss'])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Convolution Error with 2 hidden layers: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23a55e0d940>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23a555827b8>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23a55e12278>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23a55e12860>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHVCAYAAADywj0dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23a55582710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['acc'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.plot(results.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.5527589864730835, 0.4491]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://cambridgespark.com/content/tutorials/convolutional-neural-networks-with-keras/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # in each iteration, we consider 32 training examples at once\n",
    "num_epochs = 200 # we iterate 200 times over the entire training set\n",
    "kernel_size = 3 # we will use 3x3 kernels throughout\n",
    "pool_size = 2 # we will use 2x2 pooling throughout\n",
    "conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n",
    "conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n",
    "drop_prob_1 = 0.25 # dropout after pooling with probability 0.25\n",
    "drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5\n",
    "hidden_size = 512 # the FC layer will have 512 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution2c_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (1, 1), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(Conv2D(32, (1, 1), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = convolution2c_model()\n",
    "# Fit the model\n",
    "results = model.fit(X_train, y_train, validation_split=0.1, epochs=20, batch_size=1000, verbose=1)\n",
    "plt.plot(results.history['loss'])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Convolution Error with 2 hidden layers: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "40000/40000 [==============================] - 188s - loss: 2.2569 - acc: 0.1401 - val_loss: 2.0831 - val_acc: 0.2112\n",
      "Epoch 2/20\n",
      "40000/40000 [==============================] - 187s - loss: 2.0371 - acc: 0.2528 - val_loss: 1.9771 - val_acc: 0.2858\n",
      "Epoch 3/20\n",
      "40000/40000 [==============================] - 172s - loss: 1.9618 - acc: 0.2895 - val_loss: 1.9126 - val_acc: 0.3121\n",
      "Epoch 4/20\n",
      "40000/40000 [==============================] - 177s - loss: 1.8903 - acc: 0.3172 - val_loss: 1.8360 - val_acc: 0.3430\n",
      "Epoch 5/20\n",
      "40000/40000 [==============================] - 168s - loss: 1.8336 - acc: 0.3391 - val_loss: 1.7885 - val_acc: 0.3602\n",
      "Epoch 6/20\n",
      "40000/40000 [==============================] - 175s - loss: 1.7958 - acc: 0.3573 - val_loss: 1.7617 - val_acc: 0.3783\n",
      "Epoch 7/20\n",
      "40000/40000 [==============================] - 190s - loss: 1.7660 - acc: 0.3683 - val_loss: 1.7174 - val_acc: 0.3926\n",
      "Epoch 8/20\n",
      "40000/40000 [==============================] - 174s - loss: 1.7400 - acc: 0.3787 - val_loss: 1.6971 - val_acc: 0.3954\n",
      "Epoch 9/20\n",
      "40000/40000 [==============================] - 186s - loss: 1.7192 - acc: 0.3850 - val_loss: 1.6750 - val_acc: 0.4030\n",
      "Epoch 10/20\n",
      "40000/40000 [==============================] - 177s - loss: 1.6900 - acc: 0.3954 - val_loss: 1.6381 - val_acc: 0.4113\n",
      "Epoch 11/20\n",
      "40000/40000 [==============================] - 179s - loss: 1.6706 - acc: 0.4022 - val_loss: 1.6228 - val_acc: 0.4183\n",
      "Epoch 12/20\n",
      "40000/40000 [==============================] - 176s - loss: 1.6354 - acc: 0.4136 - val_loss: 1.5916 - val_acc: 0.4294\n",
      "Epoch 13/20\n",
      "40000/40000 [==============================] - 174s - loss: 1.6244 - acc: 0.4170 - val_loss: 1.5899 - val_acc: 0.4334\n",
      "Epoch 14/20\n",
      "40000/40000 [==============================] - 180s - loss: 1.6093 - acc: 0.4236 - val_loss: 1.5692 - val_acc: 0.4322\n",
      "Epoch 15/20\n",
      "40000/40000 [==============================] - 173s - loss: 1.5927 - acc: 0.4302 - val_loss: 1.5567 - val_acc: 0.4406\n",
      "Epoch 16/20\n",
      "40000/40000 [==============================] - 168s - loss: 1.5771 - acc: 0.4366 - val_loss: 1.5349 - val_acc: 0.4462\n",
      "Epoch 17/20\n",
      "40000/40000 [==============================] - 168s - loss: 1.5637 - acc: 0.4371 - val_loss: 1.5326 - val_acc: 0.4492\n",
      "Epoch 18/20\n",
      "40000/40000 [==============================] - 170s - loss: 1.5423 - acc: 0.4461 - val_loss: 1.5273 - val_acc: 0.4509\n",
      "Epoch 19/20\n",
      "40000/40000 [==============================] - 192s - loss: 1.5374 - acc: 0.4501 - val_loss: 1.5014 - val_acc: 0.4610\n",
      "Epoch 20/20\n",
      "40000/40000 [==============================] - 609s - loss: 1.5243 - acc: 0.4549 - val_loss: 1.4995 - val_acc: 0.4606\n",
      " 9984/10000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4842249984741211, 0.46929999999999999]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolution Error with 2 hidden layers: 53.07%\n"
     ]
    }
   ],
   "source": [
    "def convolution3_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = convolution3_model()\n",
    "# Fit the model\n",
    "results = model.fit(X_train, y_train, validation_split=0.2, epochs=20, batch_size=1000, verbose=1)\n",
    "plt.plot(results.history['loss'])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "scores\n",
    "print(\"Convolution Error with 2 hidden layers: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use scikit-learn to grid search the batch size and epochs\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Function to create model, required for KerasClassifier\n",
    "def convolution3_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "# # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# numpy.random.seed(seed)\n",
    "# # load dataset\n",
    "# dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# # split into input (X) and output (Y) variables\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=convolution3_model, verbose=1)\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 50, 100, 250, 500, 1000]\n",
    "epochs = [20, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was adapted from:\n",
    "https://notebooks.azure.com/anon-yqopra/libraries/pydata/html/workspace.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images_and_labels(images, labels):\n",
    "    \"\"\"Display the first image of each label.\"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    i = 1\n",
    "    for label in unique_labels:\n",
    "        # Pick the first image for each label.\n",
    "        image = images[labels.index(label)]\n",
    "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n",
    "        i += 1\n",
    "        _ = plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display_images_and_labels(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
