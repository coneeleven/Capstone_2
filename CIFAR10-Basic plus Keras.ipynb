{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using TensorFlow\n",
    "\n",
    "### Springboard Career Track Capstone Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquired from https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tar = tarfile.open(\"cifar-10-python.tar.gz\")\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create function to unpickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"  \n",
    "    from sklearn.preprocessing import OneHotEncoder, LabelBinarizer\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(range(10))\n",
    "\n",
    "    return encoder.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign batch names, unpickle batches, and concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = 'cifar-10-batches-py\\data_batch_1'\n",
    "f2 = 'cifar-10-batches-py\\data_batch_2'\n",
    "f3 = 'cifar-10-batches-py\\data_batch_3'\n",
    "f4 = 'cifar-10-batches-py\\data_batch_4'\n",
    "f5 = 'cifar-10-batches-py\\data_batch_5'\n",
    "x_test = 'cifar-10-batches-py\\\\test_batch'\n",
    "label_names = 'cifar-10-batches-py\\\\batches.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = unpickle(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y1 = b1.get(b'labels')\n",
    "b1data = b1.get(b'data')\n",
    "\n",
    "b2 = unpickle(f2)\n",
    "train_y2 = b2.get(b'labels')\n",
    "b2data = b2.get(b'data')\n",
    "\n",
    "b3 = unpickle(f3)\n",
    "train_y3 = b3.get(b'labels')\n",
    "b3data = b3.get(b'data')\n",
    "\n",
    "b4 = unpickle(f4)\n",
    "train_y4 = b4.get(b'labels')\n",
    "b4data = b4.get(b'data')\n",
    "\n",
    "b5 = unpickle(f5)\n",
    "train_y5 = b5.get(b'labels')\n",
    "b5data = b5.get(b'data')\n",
    "\n",
    "x_train = np.concatenate((b1data, b2data, b3data, b4data, b5data), axis=0)\n",
    "y_train = np.concatenate((train_y1, train_y2, train_y3, train_y4, train_y5), axis=0)\n",
    "\n",
    "y_train_cls = y_train\n",
    "y_train = one_hot_encode(y_train)\n",
    "\n",
    "test = unpickle(x_test)\n",
    "x_test = test.get(b'data')\n",
    "y_test_cls = test.get(b'labels')\n",
    "y_test = one_hot_encode(y_test_cls)\n",
    "label_names = unpickle(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  43,  50, ..., 140,  84,  72],\n",
       "       [154, 126, 105, ..., 139, 142, 144],\n",
       "       [255, 253, 253, ...,  83,  83,  84],\n",
       "       ..., \n",
       "       [ 35,  40,  42, ...,  77,  66,  50],\n",
       "       [189, 186, 185, ..., 169, 171, 171],\n",
       "       [229, 236, 234, ..., 173, 162, 161]], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_cases_per_batch': 10000,\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Example from https://github.com/martin-gorner/tensorflow-mnist-tutorial/blob/master/mnist_1.0_softmax.py\n",
    " https://docs.google.com/presentation/d/1TVixw6ItiZ8igjp6U17tcgoFrLSaHWQmMOwjlgQY9co/pub?slide=id.g110257a6da_0_598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 1 - Not convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = .001\n",
    "training_iteration = 30\n",
    "batch_size = 100\n",
    "display_step = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize placeholder variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 3072])\n",
    "W = tf.Variable(tf.zeros([3072,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, 10]) #placeholder from the true values\n",
    "y_true_cls = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(x, W) + b \n",
    "y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(y_pred,dimension=1)\n",
    "\n",
    "# loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y_true)\n",
    "cost = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# Add summary ops to collect data\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# % of correct answers found in batch\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    # load batch of images and correct answers\n",
    "    batch_X, batch_Y,batch_Y_cls = x_train,y_train,y_train_cls\n",
    "    feed_dict_train = {x: batch_X, y_true: batch_Y}\n",
    "    feed_dict_train1 = {x: batch_X, y_true: batch_Y,y_true_cls: batch_Y_cls}\n",
    "    \n",
    "    # train\n",
    "    sess.run(optimizer, feed_dict=feed_dict_train)\n",
    "    a,c = sess.run([accuracy, cross_entropy], feed_dict=feed_dict_train1)\n",
    "    a\n",
    "    \n",
    "feed_dict_test = {x: x_test,\n",
    "                  y_true: y_test,\n",
    "                  y_true_cls: y_test_cls}\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sess.run(y_pred_cls, feed_dict=feed_dict_test)    \n",
    "pred = sess.run(y_pred_cls, feed_dict=feed_dict_test)    \n",
    "cm = confusion_matrix(y_true=y_test_cls,y_pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Keras (adapted from https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=(None, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "# Keras layers can be called on TensorFlow tensors:\n",
    "x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "\n",
    "from keras.objectives import categorical_crossentropy\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        batch = x_train.next_batch(50)\n",
    "        train_step.run(feed_dict={img: x_train,\n",
    "                                  labels: y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "batch_size = 500\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(100):\n",
    "        j = 0\n",
    "        x_train_batch = x_train[j:j+batch_size,]\n",
    "        y_train_batch = y_train[j:j+batch_size,]\n",
    "        j = j + batch_size\n",
    "        if j > x_train.shape[0]:\n",
    "            j = 0\n",
    "            \n",
    "        train_step.run(feed_dict={img: x_train_batch,\n",
    "                                  labels: y_train_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print(acc_value.eval(feed_dict={img: x_test,\n",
    "                                    labels: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using baseline model Keras example from https://machinelearningmastery.com/handwritten-digit-recognition-using-convolutional-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import keras.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "83s - loss: 14.5071 - acc: 0.0999 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 2/10\n",
      "83s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 3/10\n",
      "83s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 4/10\n",
      "83s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 5/10\n",
      "83s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 6/10\n",
      "82s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 7/10\n",
      "82s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 8/10\n",
      "83s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 9/10\n",
      "89s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n",
      "Epoch 10/10\n",
      "87s - loss: 14.5063 - acc: 0.1000 - val_loss: 14.5063 - val_acc: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a11b1a400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 90.00%\n"
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with normalized inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "100s - loss: 1.9713 - acc: 0.3007 - val_loss: 1.8439 - val_acc: 0.3543\n",
      "Epoch 2/10\n",
      "98s - loss: 1.8009 - acc: 0.3653 - val_loss: 1.7556 - val_acc: 0.3815\n",
      "Epoch 3/10\n",
      "91s - loss: 1.7218 - acc: 0.3931 - val_loss: 1.6891 - val_acc: 0.4046\n",
      "Epoch 4/10\n",
      "91s - loss: 1.6639 - acc: 0.4138 - val_loss: 1.6320 - val_acc: 0.4263\n",
      "Epoch 5/10\n",
      "92s - loss: 1.6177 - acc: 0.4308 - val_loss: 1.6055 - val_acc: 0.4338\n",
      "Epoch 6/10\n",
      "91s - loss: 1.5834 - acc: 0.4419 - val_loss: 1.5750 - val_acc: 0.4508\n",
      "Epoch 7/10\n",
      "91s - loss: 1.5525 - acc: 0.4542 - val_loss: 1.5488 - val_acc: 0.4580\n",
      "Epoch 8/10\n",
      "91s - loss: 1.5276 - acc: 0.4629 - val_loss: 1.5307 - val_acc: 0.4612\n",
      "Epoch 9/10\n",
      "92s - loss: 1.5062 - acc: 0.4709 - val_loss: 1.5084 - val_acc: 0.4707\n",
      "Epoch 10/10\n",
      "91s - loss: 1.4866 - acc: 0.4771 - val_loss: 1.4926 - val_acc: 0.4792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a19bf1be0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 52.08%\n"
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3\n",
    "num_classes = 10\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "93s - loss: 2.0939 - acc: 0.2115 - val_loss: 1.9345 - val_acc: 0.2927\n",
      "Epoch 2/10\n",
      "89s - loss: 1.9539 - acc: 0.2811 - val_loss: 1.8822 - val_acc: 0.3268\n",
      "Epoch 3/10\n",
      "89s - loss: 1.9096 - acc: 0.3003 - val_loss: 1.8281 - val_acc: 0.3425\n",
      "Epoch 4/10\n",
      "89s - loss: 1.8751 - acc: 0.3154 - val_loss: 1.8142 - val_acc: 0.3445\n",
      "Epoch 5/10\n",
      "89s - loss: 1.8476 - acc: 0.3290 - val_loss: 1.7617 - val_acc: 0.3656\n",
      "Epoch 6/10\n",
      "89s - loss: 1.8224 - acc: 0.3366 - val_loss: 1.7270 - val_acc: 0.3800\n",
      "Epoch 7/10\n",
      "88s - loss: 1.7978 - acc: 0.3505 - val_loss: 1.7152 - val_acc: 0.3812\n",
      "Epoch 8/10\n",
      "89s - loss: 1.7811 - acc: 0.3559 - val_loss: 1.6802 - val_acc: 0.4033\n",
      "Epoch 9/10\n",
      "89s - loss: 1.7612 - acc: 0.3641 - val_loss: 1.6617 - val_acc: 0.4070\n",
      "Epoch 10/10\n",
      "89s - loss: 1.7494 - acc: 0.3694 - val_loss: 1.6434 - val_acc: 0.4135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a20fe1a58>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 58.65%\n"
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3   # 3072\n",
    "num_classes = 10\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# define baseline model\n",
    "def baseline_plus_hidden():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_plus_hidden()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "97s - loss: 2.2646 - acc: 0.1595 - val_loss: 2.1757 - val_acc: 0.2011\n",
      "Epoch 2/10\n",
      "91s - loss: 2.1155 - acc: 0.2015 - val_loss: 2.0545 - val_acc: 0.2521\n",
      "Epoch 3/10\n",
      "90s - loss: 2.0453 - acc: 0.2362 - val_loss: 1.9957 - val_acc: 0.2803\n",
      "Epoch 4/10\n",
      "90s - loss: 2.0002 - acc: 0.2583 - val_loss: 1.9531 - val_acc: 0.2946\n",
      "Epoch 5/10\n",
      "89s - loss: 1.9686 - acc: 0.2740 - val_loss: 1.9255 - val_acc: 0.3062\n",
      "Epoch 6/10\n",
      "89s - loss: 1.9516 - acc: 0.2825 - val_loss: 1.9076 - val_acc: 0.3120\n",
      "Epoch 7/10\n",
      "89s - loss: 1.9357 - acc: 0.2923 - val_loss: 1.8935 - val_acc: 0.3189\n",
      "Epoch 8/10\n",
      "89s - loss: 1.9218 - acc: 0.2991 - val_loss: 1.8787 - val_acc: 0.3208\n",
      "Epoch 9/10\n",
      "89s - loss: 1.9071 - acc: 0.3066 - val_loss: 1.8670 - val_acc: 0.3248\n",
      "Epoch 10/10\n",
      "90s - loss: 1.8976 - acc: 0.3110 - val_loss: 1.8522 - val_acc: 0.3332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a216e17b8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Error: 66.68%\n"
     ]
    }
   ],
   "source": [
    "num_pixels = 32 * 32 * 3   # 3072\n",
    "num_classes = 10\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# define baseline model\n",
    "def baseline_plus_hidden():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_plus_hidden()\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "# from keras import backend as K\n",
    "# K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data\n",
    "# (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "# # reshape to be [samples][pixels][width][height]\n",
    "# X_train = X_train.reshape(X_train.shape[0], 3, 32, 32).astype('float32')\n",
    "# X_test = X_test.reshape(X_test.shape[0], 3, 32, 32).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # normalize inputs from 0-255 to 0-1\n",
    "# x_train = x_train / 255\n",
    "# x_test = x_test / 255\n",
    "# # one hot encode outputs\n",
    "# y_train = np_utils.to_categorical(y_train)\n",
    "# y_test = np_utils.to_categorical(y_test)\n",
    "# num_classes = y_train.shape[1]\n",
    "# num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convolution_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(32, 32, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    # Compile model\n",
    "    adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.reshape(50000, 32, 32, 3)\n",
    "X_test = x_test.reshape(10000, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv2d_9: expected ndim=4, found ndim=3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-c816480870f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# build the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvolution_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Final evaluation of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-205a187fc5f7>\u001b[0m in \u001b[0;36mconvolution_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# create model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3072\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# and create the node connecting the current layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[1;31m# to the input layer we just created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m                 \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    455\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer conv2d_9: expected ndim=4, found ndim=3"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = convolution_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was adapted from:\n",
    "https://notebooks.azure.com/anon-yqopra/libraries/pydata/html/workspace.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train), len(y_train), len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images_and_labels(images, labels):\n",
    "    \"\"\"Display the first image of each label.\"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    i = 1\n",
    "    for label in unique_labels:\n",
    "        # Pick the first image for each label.\n",
    "        image = images[labels.index(label)]\n",
    "        plt.subplot(8, 8, i)  # A grid of 8 rows x 8 columns\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Label {0} ({1})\".format(label, labels.count(label)))\n",
    "        i += 1\n",
    "        _ = plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_and_labels(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
